# Designing Experiments

## Review
1. What is the difference between **independent** and **dependent** variables?

     The independent variable is the variable the experimenter manipulates or changes (or observes), and is assumed to have an effect on the dependent variable. The dependent variable is the variable being tested and measured in an experiment, and is, theoretically, affected by changes in the independent variable.

2. What is the difference between a **null** and **alternative** hypothesis?

     A **null** hypothesis states that there exists no relationship between the independent and dependent variables. An **alternative** hypothesis claims that there exists a relationship between the two.

3. How/why can experimental studies show causation?

     If we can show that changes of an independent variable change a dependent variable (and we seek to minimize external factors), experimental studies can show causation. Without rigorous verification, it's just correlation.

## Running Experiments
1. How/why can experimental studies show causation?

     Answered above. 

2. How might sampling bias affect the results of an experiment?

     When our sampling frame is unexpectedly different from our population, we can receive specific results that misrepresent the real population.

3. What might go wrong while conducting an experiment?

     In addition to irresponsible sampling procedures, a literal infinitude of factors could complicate an experiment's results. Our duty as experimenters is to minimize those potential sources of error and, if inevitable, account for them (or at least acknowledge them).

## Designing an Experiment
1. Put these steps in the correct order:
     - Do background research
     - Identify population
     - Identify independent and dependent variables/Identify **treatments**
     - Ask a precise question/Identify null and alternative hypotheses
     - Choose sampling method
     - Identify sample frame
     - Obtain sample to include in experiment
     - Conduct treatments on **control** and **experimental** groups
2. What are some examples of how an experiment can go wrong? Refer to specific steps from the list above.
     
     Following the same steps as above:
     - Background research can be flawed and lead you to have an incorrect theoretical background for a project
     - Your question can be focusing on the wrong phenomenon
     - You can identify a poor population for your experiment
     - You can focus on the wrong variables
     - You can pursue the wrong conjecture
     - You can develop a flawed treatment that doesn't actually test what you want to test
     - You can develop a sampling method that is irrelevant for your project
     - Your sample frame may not have the best subset of the population for your experiment
     - The sample that you obtain can be flawed or simply the wrong sample for your purposes
     - You can mess up your treatments (cross contamination, wrong procedure, etc) or mis-record data
     
3. How might an error affect the trustworthiness of a study? How much *should* an error affect the trustworthiness of a study?

     If a study is rife with error, it suggests that the experimenters were less rigorous than they could have been or that they failed to control for confounding factors. If a study acknowledges its sources of error and does its best to minimize them, it suggests that the researchers did a better job at isolating the factors of interest. Error's effect on a study's trustworthiness is dependent on the situation: if a researcher is using an experimental method for the first time ever, it would make sense that they have a larger margin of error than someone running a routine, established protocol.

4. What might a scientist do to prevent such errors?

     As with most things in life, the first step is acknowledgement. The more sources of error that a scientist can identify, the more they can actively seek to eliminate or compensate for. The fewer external phenomena that act on the experiment, the better. For example, if you're testing the lifespan of mice undergoing a treatment, you should give all mice the same food and make sure that they do not fight one another, which could cause bodily injury or death.

5. What does it mean to **double blind** a study?

     A double blind study is a randomized clinical trial in which neither the treatment administrator nor the subject know if the subject is receiving the experimental treatment, a standard treatment or a placebo. Only those directing the study know the treatment that each participant receives. Double-blind studies prevent bias when doctors (involved in treatment) evaluate patientsâ€™ outcomes. This improves reliability of clinical trial results.


