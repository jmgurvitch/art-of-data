# Designing Experiments

## Review
1. What is the difference between **independent** and **dependent** variables?

     The independent variable is the variable the experimenter manipulates or changes, and is assumed to have a direct effect on the dependent variable. The dependent variable is the variable being tested and measured in an experiment, and is, theoretically, affected by changes in the independent variable.

2. What is the difference between a **null** and **alternative** hypothesis?

     A **null** hypothesis is a statement about the population that either is believed to be true or is used to put forth an argument unless it can be shown to be incorrect beyond a reasonable doubt. An **alternative** hypothesis is a claim about the population that is contradictory to the null hypothesis and what we conclude when we reject the null hypothesis.

3. How/why can experimental studies show causation?

     If we can confirm with a high degree of certainty that changes of an independent variable change a dependent variable, experimental studies can show causation. Without rigorous verification, it's just correlation.

## Running Experiments
1. How/why can experimental studies show causation?

     Answered above. 

2. How might sampling bias affect the results of an experiment?

     Sampling bias is a bias in which a sample is collected in such a way that some members of the intended population have a lower or higher sampling probability than others. It results in a biased sample of a population in which all individuals, or instances, were not equally likely to have been selected, meaning that the conditions of the experiment could be misrepresented by the results.

3. What might go wrong while conducting an experiment?

     In addition to irresponsible sampling procedures, a literal infinitude of factors could complicate an experiment's results. Our duty as experimenters is to minimize those potential sources of error and, if inevitable, account for them (or at least acknowledge them).

## Designing an Experiment
1. Put these steps in the correct order:
     - Do background research
     - Ask a precise question
     - Identify population
     - Identify independent and dependent variables
     - Identify null and alternative hypotheses
     - Identify **treatments**
     - Choose sampling method
     - Identify sample frame
     - Obtain sample to include in experiment
     - Conduct treatments on **control** and **experimental** groups
2. What are some examples of how an experiment can go wrong? Refer to specific steps from the list above.
     
     Following the same steps as above:
     - Background research can be flawed and lead you to have an incorrect theoretical background for a project
     - Your question can be focusing on the wrong phenomenon
     - You can identify a poor population for your experiment
     - You can focus on the wrong variables
     - You can pursue the wrong conjecture
     - You can develop a flawed treatment that doesn't actually test what you want to test
     - You can develop a sampling method that is irrelevant for your project
     - Your sample frame may not have the best subset of the population for your experiment
     - The sample that you obtain can be flawed or simply the wrong sample for your purposes
     - You can mess up your treatments (cross contamination, wrong procedure, etc) or mis-record data
     
3. How might an error affect the trustworthiness of a study? How much *should* an error affect the trustworthiness of a study?

     If a study is rife with error, it suggests that the experimenters were less rigorous than they could have been or that they failed to control for confounding factors. If a study acknowledges its sources of error and does its best to minimize them, it suggests that the researchers did a better job at isolating the factors of interest. Error's effect on a study's trustworthiness is dependent on the situation: if a researcher is using an experimental method for the first time ever, it would make sense that they have a larger margin of error than someone running a routine, established protocol.

4. What might a scientist do to prevent such errors?

     As with most things in life, the first step is acknowledgement. The more sources of error that a scientist can identify, the more they can actively seek to eliminate or compensate for. The fewer external phenomena that act on the experiment, the better. For example, if you're testing the lifespan of mice undergoing a treatment, you should give all mice the same food and make sure that they do not fight one another, which could cause bodily injury or death.

5. What does it mean to **double blind** a study?

     A double blind study is a randomized clinical trial in which neither the treatment administrator nor the subject know if the subject is receiving the experimental treatment, a standard treatment or a placebo. Only those directing the study know the treatment that each participant receives. Double-blind studies prevent bias when doctors (involved in treatment) evaluate patientsâ€™ outcomes. This improves reliability of clinical trial results.


