# Designing Experiments

## Review
1. What is the difference between **independent** and **dependent** variables?

     The independent variable is the variable the experimenter manipulates or changes, and is assumed to have a direct effect on the dependent variable. The dependent variable is the variable being tested and measured in an experiment, and is, theoretically, affected by changes in the independent variable.

2. What is the difference between a **null** and **alternative** hypothesis?

     A **null** hypothesis is a statement about the population that either is believed to be true or is used to put forth an argument unless it can be shown to be incorrect beyond a reasonable doubt. An **alternative** hypothesis is a claim about the population that is contradictory to the null hypothesis and what we conclude when we reject the null hypothesis.

3. How/why can experimental studies show causation?

     If we can confirm with a high degree of certainty that changes of an independent variable change a dependent variable, experimental studies can show causation. Without rigorous verification, it's just correlation.

## Running Experiments
1. How/why can experimental studies show causation?

     Answered above. 

2. How might sampling bias affect the results of an experiment?

     Sampling bias is a bias in which a sample is collected in such a way that some members of the intended population have a lower or higher sampling probability than others. It results in a biased sample of a population in which all individuals, or instances, were not equally likely to have been selected, meaning that the conditions of the experiment could be misrepresented by the results.

3. What might go wrong while conducting an experiment?

     In addition to irresponsible sampling procedures, a literal infinitude of factors could complicate an experiment's results. Our duty as experimenters is to minimize those potential sources of error and, if inevitable, account for them (or at least acknowledge them).

## Designing an Experiment
1. Put these steps in the correct order:
     - Do background research
     - Ask a precise question
     - Identify population
     - Identify independent and dependent variables
     - Identify null and alternative hypotheses
     - Identify **treatments**
     - Choose sampling method
     - Identify sample frame
     - Obtain sample to include in experiment
     - Conduct treatments on **control** and **experimental** groups
2. What are some examples of how an experiment can go wrong? Refer to specific steps from the list above.
     
     Following the same steps as above:
     - Background research can be flawed and lead you to have an incorrect theoretical background for a project
     - Your question can be focusing on the wrong phenomenon
     - You can identify a poor population for your experiment
     - You can focus on the wrong variables
     - You can pursue the wrong conjecture
     - You can develop a flawed treatment that doesn't actually test what you want to test
     - You can develop a sampling method that is irrelevant for your project
     - Your sample frame may not have the best subset of the population for your experiment
     - The sample that you obtain can be flawed or simply the wrong sample for your purposes
     - You can mess up your treatments (cross contamination, wrong procedure, etc) or mis-record data
     
3. How might an error affect the trustworthiness of a study? How much *should* an error affect the trustworthiness of a study?

     

4. What might a scientist do to prevent such errors?

     

5. What does it mean to **double blind** a study?

     
